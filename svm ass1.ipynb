{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf394afe-50a0-4195-8f34-3741fd7c47e1",
   "metadata": {},
   "source": [
    "#Q1.\n",
    "\n",
    "A linear Support Vector Machine (SVM) is a supervised machine learning algorithm used for binary classification. It aims to find a hyperplane that best separates the data points of two classes in a way that maximizes the margin between the two classes. The mathematical formula for a linear SVM can be expressed as follows:\n",
    "\n",
    "Given a dataset of points (xi, yi) where xi is the feature vector of a data point, and yi is the corresponding class label (either +1 or -1 for binary classification), the goal is to find the hyperplane represented by the equation:\n",
    "\n",
    "w · x + b = 0\n",
    "\n",
    "where:\n",
    "\n",
    "    \"w\" is the weight vector (also called the normal vector) perpendicular to the hyperplane.\n",
    "    \"x\" is the feature vector of a data point.\n",
    "    \"b\" is the bias or intercept term.\n",
    "\n",
    "The decision function for classifying a new data point is based on the sign of the following expression:\n",
    "\n",
    "f(x) = w · x + b\n",
    "\n",
    "To find the optimal hyperplane, the SVM aims to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class. This can be expressed as:\n",
    "\n",
    "Margin = 2 / ||w||\n",
    "\n",
    "Where ||w|| represents the Euclidean norm (magnitude) of the weight vector \"w.\"\n",
    "\n",
    "The optimization problem for a linear SVM can be stated as follows:\n",
    "\n",
    "    Maximize the margin: Maximize 2 / ||w||.\n",
    "\n",
    "    Subject to the constraint that for all data points (xi, yi):\n",
    "\n",
    "    yi * (w · xi + b) >= 1 if yi is in class +1\n",
    "    yi * (w · xi + b) <= -1 if yi is in class -1\n",
    "\n",
    "This formulation essentially ensures that data points are correctly classified and that they lie outside a certain margin on the correct side of the hyperplane. The optimization problem is often solved using various techniques, such as quadratic programming or gradient descent, to find the optimal values of \"w\" and \"b.\"\n",
    "\n",
    "In practice, a soft-margin SVM allows for some misclassification to handle non-linearly separable data, and it introduces a regularization parameter (C) that balances the trade-off between maximizing the margin and minimizing classification errors. The formula is modified to include this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15448e61-8477-4d29-963e-6337fd294e89",
   "metadata": {},
   "source": [
    "#Q2.\n",
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is a mathematical expression that the SVM aims to optimize during the training process. The primary goal of this optimization is to find the optimal hyperplane that maximizes the margin between two classes while minimizing the classification error. In the case of a linear SVM, the objective function is typically defined as a minimization problem.\n",
    "\n",
    "The objective function of a linear SVM can be expressed as:\n",
    "\n",
    "Minimize:\n",
    "(1/2) * ||w||^2 + C * Σ[over all training samples] max(0, 1 - y_i * (w · x_i + b))\n",
    "\n",
    "Where:\n",
    "\n",
    "    w is the weight vector (normal vector) perpendicular to the hyperplane.\n",
    "    C is the regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error. A smaller C encourages a larger margin but allows some misclassification, while a larger C penalizes misclassification more heavily.\n",
    "    (1/2) * ||w||^2 represents the L2 norm (Euclidean norm) of the weight vector, and it is a measure of the margin size. The SVM aims to maximize the margin, which is equivalent to minimizing this term.\n",
    "    Σ denotes the summation over all training samples (i.e., all data points in the dataset).\n",
    "    y_i is the class label for the ith training sample, which is either +1 or -1 for binary classification.\n",
    "    x_i is the feature vector of the ith training sample.\n",
    "    (w · x_i + b) is the decision function or the score for the ith training sample, which measures how well the data point is classified. If this value is positive, the sample is correctly classified; if it's negative, the sample is misclassified.\n",
    "\n",
    "The term max(0, 1 - y_i * (w · x_i + b)) is often called the \"hinge loss.\" It measures the loss incurred for misclassified data points. When the decision function (w · x_i + b) is correct (i.e., has the correct sign), this term evaluates to 0, indicating no loss. When the data point is misclassified, the loss increases linearly with the margin by a value of (1 - y_i * (w · x_i + b)).\n",
    "\n",
    "The SVM optimization problem seeks to minimize the hinge loss, which encourages correct classification while also minimizing the L2 norm of the weight vector to maximize the margin. The regularization parameter C allows for fine-tuning the trade-off between these two objectives, making the SVM robust and adaptable to various types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2cc35-2a47-4646-80c1-f88bff09e0f2",
   "metadata": {},
   "source": [
    "#Q3.\n",
    "\n",
    "The kernel trick in Support Vector Machines (SVM) is a technique used to extend the capabilities of SVMs for handling non-linearly separable data. SVMs are originally designed for linear classification problems, where they find a hyperplane to separate two classes. However, in many real-world scenarios, the data may not be linearly separable, meaning a straight line cannot effectively separate the classes. This is where the kernel trick comes into play.\n",
    "\n",
    "The kernel trick allows SVMs to implicitly map the input data into a higher-dimensional feature space where it may become linearly separable. This is achieved without explicitly computing the transformation but by using a kernel function. A kernel function calculates the dot product (inner product) between data points in the higher-dimensional space without explicitly calculating the transformation. This approach offers several benefits:\n",
    "\n",
    "    Handling Non-Linearity: With the kernel trick, SVMs can effectively model non-linear decision boundaries by implicitly mapping data into higher-dimensional spaces where linear separation becomes possible.\n",
    "\n",
    "    Efficiency: By avoiding the explicit computation of the transformation, which can be computationally expensive for high-dimensional spaces, the kernel trick can save on computational resources.\n",
    "\n",
    "The most commonly used kernel functions are:\n",
    "\n",
    "    Linear Kernel (no transformation): The original linear SVM, which separates data using a linear hyperplane.\n",
    "\n",
    "    Polynomial Kernel: Introduces non-linearity by mapping data to a higher-dimensional space using polynomial transformations.\n",
    "\n",
    "    Radial Basis Function (RBF) Kernel: Commonly used for non-linear problems. It maps data into an infinite-dimensional space, which is effective at modeling complex decision boundaries.\n",
    "\n",
    "    Sigmoid Kernel: Used to model sigmoidal decision boundaries.\n",
    "\n",
    "The choice of the kernel function and its parameters (such as the degree of a polynomial kernel or the width of an RBF kernel) can significantly impact the performance of the SVM on a specific problem. Selecting the appropriate kernel and tuning its parameters is often done through cross-validation.\n",
    "\n",
    "In summary, the kernel trick in SVM is a powerful technique that allows SVMs to handle non-linear data by implicitly mapping it into a higher-dimensional feature space. This enables SVMs to find optimal decision boundaries for a wide range of classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e6659-78fc-4895-bd0c-1ac789444eaa",
   "metadata": {},
   "source": [
    "#Q4.\n",
    "\n",
    "Support vectors are a crucial concept in Support Vector Machines (SVM), and they play a central role in defining the decision boundary and margins. These are the data points that are closest to the decision boundary (hyperplane) and have the most influence on the position and orientation of the hyperplane. Let's explore the role of support vectors with an example:\n",
    "\n",
    "Suppose you have a binary classification problem with two classes: red circles and blue squares. Your goal is to find a decision boundary that separates these two classes. In a two-dimensional feature space, this decision boundary is represented by a line (for a linear SVM).\n",
    "\n",
    "Here's a simplified example:\n",
    "\n",
    "    Red circles: (2, 2), (3, 3), (4, 4)\n",
    "    Blue squares: (7, 7), (8, 8), (9, 9)\n",
    "\n",
    "Now, when you train a linear SVM, it identifies the optimal hyperplane that maximizes the margin between the two classes. This hyperplane can be represented by the equation: w · x + b = 0, where \"w\" is the weight vector, \"x\" is the feature vector, and \"b\" is the bias term. The SVM aims to maximize the margin, which is the distance between the hyperplane and the closest data points.\n",
    "\n",
    "In this example, the support vectors would be the points that are nearest to the decision boundary. In this case, the support vectors are:\n",
    "\n",
    "    Red circle at (3, 3)\n",
    "    Blue square at (8, 8)\n",
    "\n",
    "These are the points that are closest to the decision boundary and have a \"margin\" that touches the decision boundary. Other data points are farther away from the decision boundary and do not influence the position of the hyperplane. These support vectors have a special role:\n",
    "\n",
    "    Defining the Margin: The support vectors determine the width of the margin. The distance from the decision boundary to the support vectors is equal to the margin.\n",
    "\n",
    "    Influence on the Hyperplane: The position and orientation of the hyperplane are determined by these support vectors. It is the support vectors that make the SVM maximize the margin while minimizing the classification error.\n",
    "\n",
    "    Handling Misclassification: In a soft-margin SVM, support vectors play a crucial role in handling misclassified points. The SVM allows for some misclassification but penalizes them. Support vectors are often associated with misclassified points.\n",
    "\n",
    "By focusing on the support vectors, SVMs can find an optimal decision boundary that is robust and effective for classifying new, unseen data. The choice of support vectors is what makes SVMs a powerful tool for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab53a6-ec33-4245-ab1a-fc1b28fdc52c",
   "metadata": {},
   "source": [
    "#Q5.\n",
    "\n",
    "I'm unable to provide graphical illustrations directly, but I can describe the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM with a textual explanation.\n",
    "\n",
    "    Hyperplane:\n",
    "        In a linear Support Vector Machine (SVM), the hyperplane is the decision boundary that separates two classes. It is a flat, linear surface in the feature space.\n",
    "        Example: Suppose you have two classes, red circles and blue squares, in a 2D feature space. The hyperplane might be a straight line that effectively separates these two classes.\n",
    "\n",
    "    Marginal Plane:\n",
    "        The marginal plane is the hyperplane that maximizes the margin between the two classes. The margin is the distance between the hyperplane and the nearest data points from each class.\n",
    "        Example: In the same 2D feature space with red circles and blue squares, the marginal plane is the hyperplane that is positioned to maximize the distance between the nearest red circle and the nearest blue square.\n",
    "\n",
    "    Soft Margin:\n",
    "        A soft margin SVM allows for some misclassification. It finds a hyperplane that might not perfectly separate all data points, but it aims to balance between maximizing the margin and minimizing classification errors.\n",
    "        Example: In a soft margin SVM, the decision boundary might slightly intrude into one class to accommodate some misclassified points while still maintaining a reasonable margin.\n",
    "\n",
    "    Hard Margin:\n",
    "        A hard margin SVM enforces strict separation of classes, meaning it does not allow any misclassification. It requires the data to be perfectly linearly separable, and it might not always be suitable for real-world noisy datasets.\n",
    "        Example: In a hard margin SVM, the decision boundary perfectly separates all data points without any intrusion into either class. This is only feasible when the data is linearly separable without errors.\n",
    "\n",
    "Keep in mind that graphical representations can help visualize these concepts more effectively. You can use tools like Python's scikit-learn library to create SVM models and visualize the decision boundaries, margins, and support vectors. Using real data or synthetic datasets can provide practical insights into how these concepts work in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc921287-ef79-4776-9dc4-283e1a8877a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for visualization purposes\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier on the training set\n",
    "# Try different values of the regularization parameter C\n",
    "C_values = [0.1, 1, 10]\n",
    "for C in C_values:\n",
    "    clf = SVC(C=C, kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels for the testing set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Compute the accuracy of the model on the testing set\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with C={C}: {accuracy}\")\n",
    "\n",
    "    # Plot the decision boundaries of the trained model\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap=plt.cm.Paired, marker='o', edgecolor='k')\n",
    "    plt.title(f\"SVM Decision Boundary (C={C})\")\n",
    "    plt.xlabel(iris.feature_names[0])\n",
    "    plt.ylabel(iris.feature_names[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1951b-3cf2-4843-b630-13e4471b23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus Task\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.01, num_epochs=1000, C=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.C = C\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.w = np.zeros(n)\n",
    "        self.b = 0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for i in range(m):\n",
    "                condition = y[i] * (np.dot(self.w, X[i]) + self.b)\n",
    "                if condition >= 1:\n",
    "                    self.w -= self.learning_rate * (2 * self.C * self.w)\n",
    "                else:\n",
    "                    self.w -= self.learning_rate * (2 * self.C * self.w - np.dot(X[i], y[i]))\n",
    "                    self.b -= self.learning_rate * y[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "# Example usage:\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:100, :2]  # Using only the first two features for binary classification\n",
    "y = iris.target[:100]\n",
    "y[y == 0] = -1  # Convert 0 to -1 for binary classification\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear SVM\n",
    "svm = LinearSVM(C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Create and train the scikit-learn SVM\n",
    "sklearn_svm = SVC(kernel='linear', C=1.0)\n",
    "sklearn_svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred_sklearn = sklearn_svm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the scikit-learn model\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"scikit-learn Accuracy: {accuracy_sklearn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
